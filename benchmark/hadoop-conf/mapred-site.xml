<?xml version="1.0"?>

<configuration>

        <property>
          <name>hadoop.home</name>
          <value>/mnt/hsearch/hadoop</value>
        </property>

        <property>
          <name>metadata.dir</name>
          <value>/data/0</value>
          <description>Where the NameNode metadata should be stored</description>
        </property>

	<property>
	  <name>mapred.local.dir</name>
	  <value>/data/0</value>
	  <description>
		MapReduce performance can also be improved by distributing the temporary data generated by MapReduce tasks across multiple disks on each machine:	  /d1/mapred/local,/d2/mapred/local,/d3/mapred/local,/d4/mapred/local if there are multiple drives available in the NameNode, they can be used to provide additional redundant copies of the NameNode metadata in the event of the failure of one drive. Unlike the above two properties, where one drive out of many is selected to write a piece of data, the NameNode writes to each comma-separated path in dfs.name.dir. If too many drives are listed here it may adversely affect the performance of the NameNode, as the probability of blocking on one or more I/O operations increases with the number of devices involved, but it is imperative that the sole copy of the metadata does not reside on a single drive.		</description>
	</property>

	<property>
	  <name>mapred.job.tracker</name>
	  <value>master1:8021</value>
	  <description>Job Tracker </description>
	</property>

	<property>
	  <name>mapred.child.java.opts</name>
	  <value>-Xmx256m -XX:+UseConcMarkSweepGC</value>
	  <description>Set based on the metrics</description>
	</property>

	<property>
	  <name>mapred.job.tracker.handler.count</name>
	  <value>40</value>
	  <description>With multiple racks of servers, RPC timeouts may become more frequent. The NameNode takes a continual census of DataNodes and their health via heartbeat messages sent every few seconds. A similar timeout mechanism exists on the MapReduce side with the JobTracker. With many racks of machines, they may force one another to timeout because the master node is not handling them fast enough. The following options increase the number of threads on the master machine dedicated to handling RPC's from slave nodes:
	  </description>
	</property>
	
	<property>
	  <name>io.sort.factor</name>
	  <value>10</value>
	  <description>Number of streams to merge concurrently when
		sorting files during shuffling
		For 250 to 2000 nodes , the value is , 50-200
	  </description>
	  
	</property>

	<property>
	  <name>io.sort.mb</name>
	  <value>50</value>
	  <description>Amount of memory to use while sorting data
		For 250 to 2000 nodes , the value is , 50-200
	  </description>
	</property>

	<property>
	  <name>mapred.reduce.parallel.copies</name>
	  <value>40</value>
	  <description> Number of concurrent connections a reducer
		should use when fetching its input from mappers
		For 250 to 2000 nodes , the value is , 20-50
	  </description>
	</property>

	<property>
	  <name>tasktracker.http.threads</name>
	  <value>40</value>
	  <description>Number of threads each TaskTracker uses to
		provide intermediate map output to reducers
		For 250 to 2000 nodes , the value is , 40-50
	  </description>
	</property>
	
	<property>
	  <name>mapred.tasktracker.map.tasks.maximum</name>
	  <value>12</value>
	  <description>1/2 * (cores/node) to 2 * (cores/node)
	  	Number of map tasks to deploy on each machine
	  </description>
	</property>	

	<property>
	  <name>mapred.tasktracker.reduce.tasks.maximum</name>
	  <value>12</value>
	  <description>1/2 * (cores/node) to 2 * (cores/node)
	  	Number of reduce tasks to deploy on each machine.
	  </description>
	</property>	

	<property>
	  <name>mapred.system.dir</name>
	  <value>${hadoop.tmp.dir}/mapred/system</value>
	  <description>Path on the HDFS where where the Map/Reduce framework stores system files </description>
	</property>
	
	<property>
	  <name>mapred.temp.dir</name>
	  <value>${hadoop.tmp.dir}/mapred/temp</value>
	  <description></description>
	</property>
	
	<property>
	  <name>mapred.job.tracker.persist.jobstatus.dir</name>
	  <value>${metadata.dir}/cluster/jobtracker/jobsInfo</value>
	  <description></description>
	</property>

	<property>
	  <name>mapred.hosts.exclude</name>
	  <value>${hadoop.home}/conf/excludes</value>
	  <description>Same as dfs.hosts.exclude
	  </description>
	</property>

</configuration>
