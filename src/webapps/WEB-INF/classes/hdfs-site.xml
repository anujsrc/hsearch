<?xml version="1.0"?>

<configuration>

	<property>
	  <name>dfs.replication</name>
	  <value>1</value>
	  <description>
	     Clusters closer to the 8-10 node range may want to set dfs.replication to 3. Values higher than 3 are usually
	     not necessary. Individual files which are heavily utilized by a large number of nodes may have their particular
	     replication factor manually adjusted upward independent of the cluster default
	  </description>
	</property>
	
	<property>
	  <name>dfs.block.size</name>
	  <value>33554432</value>
	  <description>The NameNode is responsible for managing metadata associated with each block in the HDFS. As the amount of
		information in the rack scales into the 10's or 100's of TB, this can grow to be quite sizable. The NameNode
		machine needs to keep the blockmap in RAM to work efficiently. Therefore, at large scale, this machine will require
		more RAM than other machines in the cluster. The amount of metadata can also be dropped almost in half by
		doubling the block size: This changes the block size from 64MB (the default) to 128MB, which decreases pressure on the NameNode's
		memory. On the other hand, this potentially decreases the amount of parallelism that can be achieved, as the
		number of blocks per file decreases. This means fewer hosts may have sections of a file to offer to MapReduce
		tasks without contending for disk access. The larger the individual files involved (or the more files involved in the
		average MapReduce job), the less of an issue this is.
	  </description>
	</property>

	<property>
	  <name>dfs.namenode.handler.count</name>
	  <value>40</value>
	  <description>With multiple racks of servers, RPC timeouts may become more frequent. The NameNode takes a continual
		census of DataNodes and their health via heartbeat messages sent every few seconds. A similar timeout
		mechanism exists on the MapReduce side with the JobTracker. With many racks of machines, they may force one
		another to timeout because the master node is not handling them fast enough. The following options increase the
		number of threads on the master machine dedicated to handling RPC's from slave nodes:
	  </description>
	</property>

	<property>
	  <name>dfs.datanode.du.reserved</name>
	  <value>1073741824</value>
	  <description> Reserve freespace on datanode </description>
	</property>
	
	<!--
	<property>
	  <name>dfs.data.dir</name>
	  <value>/data/1/sc/dfs,/data/2/sc/dfs</value>
	  <description>	All location in which data gets stored.</description>
	</property>
	-->
	
	<property>
	  <name>dfs.datanode.max.xcievers</name>
	  <value>4096</value>
	  <description>Concurrent requests</description>
	</property>

</configuration>
